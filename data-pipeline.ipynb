{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_1\"></a>\n## Keras datasets\n\nFor a list of Keras datasets and documentation on recommended usage, see [this link](https://keras.io/datasets/)."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the CIFAR-100 Dataset"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from tensorflow.keras.datasets import cifar100\n(train_images, train_labels),(test_images, test_labels) = cifar100.load_data(label_mode='fine')\n# Examine the shape of the data.\nprint(train_images.shape)\nprint(train_labels.shape)\nprint(test_images.shape) \nprint(test_labels.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Examine one of the images and its corresponding label\nj = 478\nprint(train_labels[j])\nplt.imshow(train_images[j])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import json\n\nwith open('data/cifar100_fine_labels.json', 'r') as fine_labels:\n    cifar100_fine_labels = json.load(fine_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The list of labels for the CIFAR-100 dataset are available [here](https://www.cs.toronto.edu/~kriz/cifar.html)."},{"metadata":{"trusted":false},"cell_type":"code","source":"j = 488\nprint(train_labels[j])\nprint(cifar100_fine_labels[train_labels[j][0]])\nplt.imshow(train_images[j])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the data using different label modes"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Display a few examples from category 87 (index 86) and the list of labels\n\nexamples = train_images[(train_labels.T == 25)[0]][:3]\nfig, ax = plt.subplots(1,3)\nax[0].imshow(examples[0])\nax[1].imshow(examples[1])\nax[2].imshow(examples[2])\n\n# Reload the data using the 'coarse' label mode\n(train_images, train_labels),(test_images, test_labels) = cifar100.load_data(label_mode='coarse')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Display three images from the dataset with the label 6 (index 5)\n\nexamples = train_images[(train_labels.T == 5)[0]][:3]\nfig, ax = plt.subplots(1,3)\nax[0].imshow(examples[0])\nax[1].imshow(examples[1])\nax[2].imshow(examples[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the list of coarse labels from a JSON file\n\nwith open('data/cifar100_coarse_labels.json', 'r') as coarse_labels:\n    cifar100_coarse_labels = json.load(coarse_labels)\n    \n# Print a few of the labels\nj = 488\nprint(train_labels[j])\n#print(cifar100_fine_labels[train_labels[j][0]])\nprint(cifar100_coarse_labels[train_labels[j][0]])\nplt.imshow(train_images[j])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the IMDB Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.datasets import imdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the IMDB dataset\n(train_data,train_labels),(test_data, test_labels) = imdb.load_data()\n\n# Print an example from the training dataset, along with its corresponding label\nprint(train_data[0])\nprint(train_labels[0])\n\n# Get the lengths of the input sequences\nsequence_lengths = [len(seq) for seq in train_data]\n\n# Determine the maximum and minimum sequence length\nprint(max(sequence_lengths))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Keyword Arguments"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the data ignoring the 50 most frequent words, use oov_char=2 (this is the default)\n(train_data,train_labels),(test_data, test_labels) = imdb.load_data(skip_top=50, oov_char=2)\n#skip_top = skips the most commonly used words\n##oov_char is a placeholder for any word that was skipped due to \"skip_top\".\n\n##The skip_top option does NOT delet the top frequency terms. They are just replaced by oov_char.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get the lengths of the input sequences\nsequence_lengths = [len(seq) for seq in train_data]\n\n# Determine the maximum and minimum sequence length\nprint(max(sequence_lengths))\nprint(min(sequence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##The skip_top option does NOT delet the top frequency terms. They are just replaced by oov_char.\n##To removem, use the following:\n\n# Define functions for filtering the sequences\n\ndef remove_oov_char(element):\n    ''' Filter function for removing the oov_char. '''\n    return [word for word in element if word!=2]\n\ndef filter_list(lst):\n    ''' Run remove_oov_char on elements in a list. '''\n    return [remove_oov_char(element) for element in lst]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove the oov_char from the sequences using the filter_list function\ntrain_data = filter_list(train_data)\ntest_data = filter_list(test_data)\n\n# Get the lengths of the input sequences\nsequence_lengths = [len(seq) for seq in train_data]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_2\"></a>\n## Dataset generators"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the UCI Fertility Dataset\n\nWe will be using a dataset available at https://archive.ics.uci.edu/ml/datasets/Fertility from UC Irvine."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the fertility dataset\n\nheaders = ['Season', 'Age', 'Diseases', 'Trauma', 'Surgery', 'Fever', 'Alcohol', 'Smoking', 'Sitting', 'Output']\nfertility = pd.read_csv('data/fertility_diagnosis.txt', delimiter=',', header=None, names=headers)\n\n# Print the shape of the DataFrame\nfertility.shape\n\n# Show the head of the DataFrame\nfertility.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Process the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Map the 'Output' feature from 'N' to 0 and from 'O' to 1\nfertility['Output'] = fertility['Output'].map(lambda x : 0.0 if x=='N' else 1.0)\n# Show the head of the DataFrame\nfertility.head()\n\n# Convert the DataFrame so that the features are mapped to floats\nfertility = fertility.astype('float32')\n\n# Shuffle the DataFrame\nfertility = fertility.sample(frac=1).reset_index(drop=True)\n\n# Convert the field Season to a one-hot encoded vector\nfertility = pd.get_dummies(fertility, prefix='Season', columns=['Season'])\n\n# Move the Output column such that it is the last column in the DataFrame\nfertility.columns = [col for col in fertility.columns if col != 'Output'] + ['Output']\n\n# Convert the DataFrame to a numpy array.\nfertility = fertility.to_numpy()\n\n# Show the head of the DataFrame once more to compare\nfertility.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split the Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split the dataset into training and validation set\n\ntraining = fertility[0:70]\nvalidation = fertility[70:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Verify the shape of the training data\ntraining.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Separate the features and labels for the validation and training data\n\ntraining_features = training[:,0:-1]\ntraining_labels = training[:,-1]\nvalidation_features = validation[:,0:-1]\nvalidation_labels = validation[:,-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create the Generator"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a function that returns a generator producing inputs and labels\n##YIEDS BATCHES (not indivudual elements)?\ndef get_generator(features, labels, batch_size=1):\n    for n in range(int(len(features)/batch_size)):\n        yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Apply the function to our training features and labels with a batch size of 10\n\ntrain_generator = get_generator(training_features, training_labels, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test the generator using the next() function\nnext(train_generator)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Build the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a model using Keras with 3 layers\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization\n\ninput_shape = (12,)\noutput_shape = (1,)\n\nmodel_input = Input(input_shape)\nbatch_1 = BatchNormalization(momentum=0.8)(model_input)\ndense_1 = Dense(100, activation='relu')(batch_1)\nbatch_2 = BatchNormalization(momentum=0.8)(dense_1)\noutput = Dense(1, activation='sigmoid')(batch_2)\n\nmodel = Model([model_input], output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Display the model summary to show the resultant structure\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compile the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create the optimizer object\nimport  tensorflow.keras.optimizers\noptimizer = tensorflow.keras.optimizers.Adam(learning_rate=1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compile the model with loss function and metric\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train and evaluate the model using the generator"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Calculate the number of training steps per epoch for the given batch size.\n\nbatch_size = 5\ntrain_steps = len(training) // batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Set the epochs to 3\n\nepochs = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train the model\nfor epoch in range(epochs):\n    train_generator = get_generator(training_features, \n                                    training_labels, \n                                    batch_size= batch_size)\n    \n    validation_generator = get_generator(validation_features, \n                                         validation_labels, \n                                         batch_size= 30) #30 is te actual size of valid data\n    \nmodel.fit_generator(train_generator, \n                    steps_per_epoch=train_steps,\n                    validation_data=validation_generator, \n                    validation_steps=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Try to run the fit_generator function once more; observe what happens\n\nmodel.fit_generator(train_generator, steps_per_epoch=train_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Make an infinitely looping generator"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a function that returns an infinitely looping generator\n\ndef get_generator_cyclic(features, labels, batch_size=1):\n    while True:\n        for n in range(int(len(features)/batch_size)):\n            yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])\n        permuted = np.random.permutation(len(features))\n        features = features[permuted]\n        labels = labels[permuted]\n        \n##just added 'while True' to the toriginal generator ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a generator using this function.\n\ntrain_generator_cyclic = get_generator_cyclic(training_features, training_labels, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assert that the new cyclic generator does not raise a StopIteration\n\nfor i in range(2*train_steps):\n    next(train_generator_cyclic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Generate a cyclic validation generator\n\nvalidation_generator_cyclic = get_generator_cyclic(validation_features, validation_labels, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Train the model\n\"\"\"\n\nfor epoch in range(epochs):\n    train_generator = get_generator(training_features, \n                                    training_labels, \n                                    batch_size= batch_size)\n    \n    validation_generator = get_generator(validation_features, \n                                         validation_labels, \n                                         batch_size= 30) #30 is te actual size of valid data\n\n\"\"\"    \nmodel.fit_generator(train_generator_cyclic, \n                    steps_per_epoch=train_steps,\n                    validation_data=validation_generator_cyclic, \n                    validation_steps=1,\n                   epochs = 10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model and get predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's obtain a validation data generator.\n\nvalidation_generator = get_generator(validation_features, validation_labels, batch_size=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get predictions on the validation data\npredcitions = model.predict(validation_generator_cyclic, steps=1)\nprint(np.round(predcitions).T)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Print the corresponding validation labels\n\nprint(validation_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Obtain a validation data generator\n\nvalidation_generator = get_generator(validation_features, validation_labels, batch_size=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model\nprint(model.evaluate(validation_generator)) \n\n##!!use .evaluate method instead of .evaluate_generator, since the generator will run out of the data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_3\"></a>\n## Keras image data augmentation"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the CIFAR-10 Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the CIFAR-10 dataset\n\n(training_features, training_labels), (test_features, test_labels) = cifar10.load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert the labels to a one-hot encoding\nfrom tensorflow.keras.utils import to_categorical\nnum_classes = 10\n\n#training_labels = tensorflow.keras.utils.to_categorical(training_labels, num_classes)\n#test_labels = tensorflow.keras.utils.to_categorical(test_labels, num_classes)\ntraining_labels = to_categorical(training_labels, num_classes)\ntest_labels = to_categorical(test_labels, num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a generator function"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a function that returns a data generator\n\ndef get_generator(features, labels, batch_size=1):\n    for n in range(int(len(features)/batch_size)):\n        yield (features[n*batch_size:(n+1)*batch_size], labels[n*batch_size:(n+1)*batch_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use the function we created to get a training data generator with a batch size of 1\n\ntraining_generator = get_generator(training_features, training_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assess the shape of the items generated by training_generator using the `next` function to yield an item.\n\nimage, label = next(training_generator)\nprint(image.shape)\nprint(label.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test the training generator by obtaining an image using the `next` generator function, and then using imshow to plot it.\n# Print the corresponding label\n\nfrom matplotlib.pyplot import imshow\n\nimage, label = next(training_generator)\nimage_unbatched = image[0,:,:,:]\nimshow(image_unbatched)\nprint(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Reset the generator by re-running the `get_generator` function.\n\ntrain_generator = get_generator(training_features, training_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a data augmention generator"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a function to convert an image to monochrome\n\ndef monochrome(x):\n    def func_bw(a):\n        average_colour = np.mean(a)\n        return [average_colour, average_colour, average_colour]\n    x = np.apply_along_axis(func_bw, -1, x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create an ImageDataGenerator object\n###Definition of generator:\nimage_generator=ImageDataGenerator(preprocessing_function=monochrome,\n                                  rotation_range = 180,\n                                  rescale = 1/255.)\n##applying the generator's params from our the information of data\nimage_generator.fit(training_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check [the documentation](https://keras.io/preprocessing/image/) for the full list of image data augmentation options. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create an iterable generator using the `flow` function\n###create actual generator:\nimage_generator_iterable = image_generator.flow(training_features, training_labels, batch_size=10, shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Reset the generators\n\ntrain_generator = get_generator(training_features, training_labels)\nimage_generator_iterable = image_generator.flow(training_features, training_labels, batch_size=10, shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# Show a sample from the generator and compare with the original\n\nimage, label = next(image_generator_iterable)\nimage_orig, label_orig = next(train_generator)\nfigs, axes = plt.subplots(1,2)\naxes[0].imshow(image[0,:,:,:])\naxes[0].set_title('Transformed')\naxes[1].imshow(image_orig[0,:,:,:])\naxes[1].set_title('Original')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Flow from directory"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the directory structure\n\ntrain_path = 'data/flowers-recognition-split/train'\nval_path = 'data/flowers-recognition-split/val'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create an ImageDataGenerator object\n\ndatagenerator = ImageDataGenerator(rescale=(1/255.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a training data generator\ntrain_generator = datagenerator.flow_from_directory(train_path, \n                                                     batch_size=64, \n                                                     classes=classes, \n                                                     target_size=[16,16] #limit memroy during training\n                                                    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a validation data generator\nvalidation_generator = datagenerator.flow_from_directory(val_path, \n                                                     batch_size=64, \n                                                     classes=classes, \n                                                     target_size=[16,16] #limit memroy during training\n                                                    )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get and display an image and label from the training generator\n\nx = next(train_generator)\nimshow(x[0][4])\nprint(x[1][4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Reset the training generator\ntrain_generator = datagenerator.flow_from_directory(train_path, \n                                                     batch_size=64, \n                                                     classes=classes, \n                                                     target_size=[16,16] #limit memroy during training\n                                                    )\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a model to train"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# Build a CNN model\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense\nfrom tensorflow.keras import Sequential\nmodel = Sequential()\nmodel.add(Input((16,16,3)))\nmodel.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\nmodel.add(MaxPooling2D((4,4)))\nmodel.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(4, (4, 4), padding='same', activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create an optimizer object\n\noptimizer = tf.keras.optimizers.Adam(1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compile the model\n\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the model summary\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Calculate the training generator and test generator steps per epoch\n\ntrain_steps_per_epoch = train_generator.n // train_generator.batch_size\nval_steps = val_generator.n // val_generator.batch_size\nprint(train_steps_per_epoch, val_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fit the model\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict using the generator"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict labels with the model\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_4\"></a>\n## The Dataset Class"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a simple dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"x = np.zeros((100,10,2,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a dataset from the tensor x\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the Dataset object\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x2 = [np.zeros((10,2,2)), np.zeros((5,2,2))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Try creating a dataset from the tensor x2\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x2 = [np.zeros((10,1)), np.zeros((10,1)), np.zeros((10,1))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create another dataset from the new x2 and inspect the Dataset object\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the element_spec\n\nprint(dataset2.element_spec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a zipped dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Combine the two datasets into one larger dataset\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the element_spec\n\nprint(dataset_zipped.element_spec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Define a function to find the number of batches in a dataset\n\ndef get_batches(dataset):\n    iter_dataset = iter(dataset)\n    i = 0\n    try:\n        while next(iter_dataset):\n            i = i+1\n    except:\n        return i","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find the number of batches in the zipped Dataset\n\nget_batches(dataset_zipped)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a dataset from numpy arrays"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the MNIST dataset\n\n(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n\nprint(type(train_features), type(train_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a Dataset from the MNIST data\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the Dataset object\n\nprint(mnist_dataset.element_spec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the length of an element using the take method\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Examine the shapes of the data\n\nprint(element[0].shape)\nprint(element[1].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a dataset from text data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the list of text files\n\ntext_files = sorted([f.path for f in os.scandir('data/shakespeare')])\n\nprint(text_files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the first file using python and print the first 5 lines.\n\nwith open(text_files[0], 'r') as fil:\n    contents = [fil.readline() for i in range(5)]\n    for line in contents:\n        print(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the lines from the files into a dataset using TextLineDataset\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use the take method to get and print the first 5 lines of the dataset\n\nfirst_5_lines_dataset = iter(shakespeare_dataset.take(5))\nlines = [line for line in first_5_lines_dataset]\nfor line in lines:\n    print(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compute the number of lines in the first file\n\nlines = []\nwith open(text_files[0], 'r') as fil:\n    line = fil.readline()\n    while line:\n        lines.append(line)\n        line = fil.readline()\n    print(len(lines))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compute the number of lines in the shakespeare dataset we created\n\nshakespeare_dataset_iterator = iter(shakespeare_dataset)\nlines = [line for line in shakespeare_dataset_iterator]\nprint(len(lines))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interleave lines from the text data files"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Create a dataset of the text file strings\n\ntext_files_dataset = tf.data.Dataset.from_tensor_slices(text_files)\nfiles = [file for file in text_files_dataset]\nfor file in files:\n    print(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Interleave the lines from the text files\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the first 10 elements of the interleaved dataset\n\nlines = [line for line in iter(interleaved_shakespeare_dataset.take(10))]\nfor line in lines:\n    print(line)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_5\"></a>\n## Training with Datasets"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the UCI Bank Marketing Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the CSV file into a pandas DataFrame\n\nbank_dataframe = pd.read_csv('data/bank/bank-full.csv', delimiter=';')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Show the head of the DataFrame\n\nbank_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the shape of the DataFrame\n\nprint(bank_dataframe.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Select features from the DataFrame\n\nfeatures = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n            'loan', 'contact', 'campaign', 'pdays', 'poutcome']\nlabels = ['y']\n\nbank_dataframe = bank_dataframe.filter(features + labels)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Show the head of the DataFrame\n\nbank_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preprocess the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert the categorical features in the DataFrame to one-hot encodings\n\nfrom sklearn.preprocessing import LabelBinarizer\n\nencoder = LabelBinarizer()\ncategorical_features = ['default', 'housing', 'job', 'loan', 'education', 'contact', 'poutcome']\n\nfor feature in categorical_features:\n    bank_dataframe[feature] = tuple(encoder.fit_transform(bank_dataframe[feature]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Show the head of the DataFrame\n\nbank_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Shuffle the DataFrame\n\nbank_dataframe = bank_dataframe.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create the Dataset object"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert the DataFrame to a Dataset\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the Dataset object\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Filter the Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# First check that there are records in the dataset for non-married individuals\n\ndef check_divorced():\n    bank_dataset_iterable = iter(bank_dataset)\n    for x in bank_dataset_iterable:\n        if x['marital'] != 'divorced':\n            print('Found a person with marital status: {}'.format(x['marital']))\n            return\n    print('No non-divorced people were found!')\n\ncheck_divorced()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Filter the Dataset to retain only entries with a 'divorced' marital status\n\nbank_dataset = bank_dataset.filter(lambda x : tf.equal(x['marital'], tf.constant([b'divorced']))[0] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the records in the dataset again\n\ncheck_divorced()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Map a function over the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert the label ('y') to an integer instead of 'yes' or 'no'\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the Dataset object\n\nbank_dataset.element_spec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove the 'marital' column\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the Dataset object\n\nbank_dataset.element_spec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create input and output data tuples"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create an input and output tuple for the dataset\n\ndef map_feature_label(x):\n    features = [[x['age']], [x['balance']], [x['campaign']], x['contact'], x['default'],\n                x['education'], x['housing'], x['job'], x['loan'], [x['pdays']], x['poutcome']]\n    return (tf.concat(features, axis=0), x['y'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Map this function over the dataset\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inspect the Dataset object\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split into a training and a validation set"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Determine the length of the Dataset\n\ndataset_length = 0\nfor _ in bank_dataset:\n    dataset_length += 1\nprint(dataset_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make training and validation sets from the dataset\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Build a classification model\n\nNow let's build a model to classify the features."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Build a classifier model\n\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, BatchNormalization\nfrom tensorflow.keras import Sequential\n\nmodel = Sequential()\nmodel.add(Input(shape=(30,)))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Dense(400, activation='relu'))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Dense(400, activation='relu'))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compile the model\n\noptimizer = tf.keras.optimizers.Adam(1e-4)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Show the model summary\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create batched training and validation datasets\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Shuffle the training data\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Fit the model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the training and validation accuracy\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}