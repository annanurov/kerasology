{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regularization example with IRIS dataset"},{"metadata":{},"cell_type":"markdown","source":"## Model validation on the Iris dataset"},{"metadata":{},"cell_type":"markdown","source":"### Intentions\n\nWe will implement validation, regularisation and callbacks on IRIS dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"from numpy.random import seed\nseed(8)\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, model_selection \n%matplotlib inline\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import initializers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IRIS pictures here.."},{"metadata":{},"cell_type":"markdown","source":"#### The Iris dataset\n\nIn this assignment, you will use the [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). It consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. For a reference, see the following papers:\n\n- R. A. Fisher. \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179â€“188, 1936.\n\nOur goal is to showcase a neural network that classifies each sample into the correct class, as well as applying validation and regularisation techniques."},{"metadata":{},"cell_type":"markdown","source":"#### Load and preprocess the data\n\nFirst read in the Iris dataset using `datasets.load_iris()`, and split the dataset into training and test sets."},{"metadata":{"trusted":false},"cell_type":"code","source":"def read_in_and_split_data(iris_data):\n    targets = iris_data.target\n    data = iris_data.data\n    \n    train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)\n    return train_data, test_data, train_targets, test_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"iris_data = datasets.load_iris()\ntrain_data, test_data, train_targets, test_targets = read_in_and_split_data(iris_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now convert the training and test targets using a one hot encoder."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_targets = tf.keras.utils.to_categorical(np.array(train_targets))\ntest_targets = tf.keras.utils.to_categorical(np.array(test_targets))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Build the neural network model\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_model(input_shape):\n    model=tf.keras.Sequential([\n        Dense(64, activation='relu', input_shape=(input_shape),\n             kernel_initializer=tf.keras.initializers.he_uniform(),######SHOULD we use the seed as defined above?\n             bias_initializer=initializers.Ones()),\n        Dense(128,activation='relu'),\n        Dense(128,activation='relu'),\n        Dense(128,activation='relu'),\n        Dense(128,activation='relu'),\n        \n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        \n        Dense(3, activation='softmax')\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = get_model(train_data[0].shape)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compile the model\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def compile_model(model):\n    return model.compile(loss='mse', optimizer=\"adam\", metrics=[\"mse\",\"mae\",\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"compile_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fit the model to the training data"},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_model(model, train_data, train_targets, epochs):\n    history = model.fit( train_data, train_targets, epochs=epochs, batch_size=40, validation_split=0.40)\n    return history    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"history = train_model(model, train_data, train_targets, epochs=800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the learning curves\n\nWe will now plot two graphs:\n* Epoch vs accuracy\n* Epoch vs loss\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"try:\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reducing overfitting in the model"},{"metadata":{},"cell_type":"markdown","source":"We update the above model by adding regularization:\ntwo dropout layers, weight decay, and a batch normalisation layer. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dropout\n\ndef get_regularised_model(input_shape, dropout_rate, weight_decay):\n    model=tf.keras.Sequential([\n        Dense(64, activation='relu', input_shape=(input_shape),\n             kernel_initializer=tf.keras.initializers.he_uniform(),######SHOULD we use the seed as defined above?\n             bias_initializer=initializers.Ones(),\n             kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(128,activation='relu', kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(128,activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dropout(dropout_rate),\n        Dense(128,activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(128,activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        \n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dropout(dropout_rate),\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        \n        Dense(3, activation='softmax')\n    ])\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Instantiate, compile and train the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"reg_model = get_regularised_model(train_data[0].shape, 0.3, 0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"compile_model(reg_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reg_history = train_model(reg_model, train_data, train_targets, epochs=800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the learning curves"},{"metadata":{"trusted":false},"cell_type":"code","source":"try:\n    plt.plot(reg_history.history['accuracy'])\n    plt.plot(reg_history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(reg_history.history['acc'])\n    plt.plot(reg_history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(reg_history.history['loss'])\nplt.plot(reg_history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the regularisation has helped to reduce the overfitting of the network.\nLet's incorporate callbacks into a new training run that implements early stopping and learning rate reduction on plateaux.\n\nWe wrote a functon so that:\n\n* It creates an `EarlyStopping` callback object and a `ReduceLROnPlateau` callback object\n* The early stopping callback is used and monitors validation loss with the mode set to `\"min\"` and patience of 30.\n* The learning rate reduction on plateaux is used with a learning rate factor of 0.2 and a patience of 20."},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_callbacks():\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min')\n    learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20)\n    return early_stopping, learning_rate_reduction    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"call_model = get_regularised_model(train_data[0].shape, 0.3, 0.0001)\ncompile_model(call_model)\nearly_stopping, learning_rate_reduction = get_callbacks()\ncall_history = call_model.fit(train_data, train_targets, epochs=800, validation_split=0.15,\n                         callbacks=[early_stopping, learning_rate_reduction], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(learning_rate_reduction.patience)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's replot the accuracy and loss graphs for our new model."},{"metadata":{"trusted":false},"cell_type":"code","source":"try:\n    plt.plot(call_history.history['accuracy'])\n    plt.plot(call_history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(call_history.history['acc'])\n    plt.plot(call_history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(call_history.history['loss'])\nplt.plot(call_history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model on the test set\n\n#test_loss, test_acc = call_model.evaluate(test_data, test_targets, verbose=0)\nresults = call_model.evaluate(test_data, test_targets, verbose=0)\n#contents of 'results' is loss vaule and  metrics which are [\"mse\",\"mae\",\"accuracy\"], as set during compilie stage.\n\n#print(\"Test loss: {:.3f}\\nTest accuracy: {:.2f}%\".format(test_loss, 100 * test_acc))\nprint(\"Test loss: {:.3f}\\nTest accuracy: {:.2f}%\".format(results[0], 100 * results[3]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}