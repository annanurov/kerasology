{"cells":[{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation, regularisation and callbacks"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_1\"></a>\n## Validation sets"},{"metadata":{},"cell_type":"markdown","source":"#### Load the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the diabetes dataset\nfrom sklearn.datasets import load_diabetes\n\ndiabetes_dataset = load_diabetes()\nprint (diabetes_dataset[\"DESCR\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Save the input and target variables\n#print(diabetes_dataset.keys())\n\ndata = diabetes_dataset[\"data\"]\ntargets = diabetes_dataset[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Normalise the target data (this will make clearer training curves)\n\n#targets.mean(axis=0)\ntargets = ( targets - targets.mean(axis=0) )/ targets.std()\n\ntargets.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split the data into train and test sets\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_data, test_data ,train_targets, test_targets = train_test_split(data, targets, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)\nprint(train_targets.shape)\nprint(test_targets.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train a feedforward neural network model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Build the model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten#, Conv2D, MaxPooling2D\nhelp(Dense)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_model():\n    model=Sequential([\n        Dense (128, activation='relu',input_shape=(train_data.shape[1],), name=\"dense1\"),\n        Dense (128, activation='relu', name=\"dense2\"),\n        Dense (128, activation='relu', name=\"dense3\"),\n        Dense (128, activation='relu', name=\"dense4\"),\n        Dense (128, activation='relu', name=\"dense5\"),\n        Dense (128, activation='relu', name=\"dense6\"),\n        Dense (1, activation='relu', name=\"dense7\")\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print the model summary\nmodel = get_model()\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compile the model\n\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train the model, with some of the data reserved for validation\nhistory = model.fit(train_data, train_targets, epochs=100, \n          validation_split=0.15, \n          batch_size=64, \n          verbose=2) #verbose=False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model on the test set\nmodel.evaluate(test_data , test_targets, verbose=0 )\n#dir(model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the learning curves"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the training and validation loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_2\"></a>\n## Model regularisation"},{"metadata":{},"cell_type":"markdown","source":"#### Adding regularisation with weight decay and dropout"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_regularised_model(wd, rate):\n    model = Sequential([\n        Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(wd), input_shape=(train_data.shape[1],)),\n        Dropout(rate),\n        Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(wd)),\n        Dropout(rate),\n        Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(wd)),\n        Dropout(rate),\n        Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(wd)),\n        Dropout(rate),\n        Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(wd)),\n        Dropout(rate),\n        Dense(128, activation=\"relu\"),\n        Dropout(rate),\n        Dense(1)\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-build the model with weight decay and dropout layers\n\nrmodel = get_regularised_model(0.00001, 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compile the model\n\nrmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train the model, with some of the data reserved for validation\nhistory = rmodel.fit(train_data, train_targets, epochs=100, \n          validation_split=0.15, \n          batch_size=64, \n          verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model on the test set\nrmodel.evaluate(test_data , test_targets, verbose=0)\n#history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the learning curves"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the training and validation loss\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_3\"></a>\n## Introduction to callbacks"},{"metadata":{},"cell_type":"markdown","source":"#### Example training callback"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Write a custom callback\nfrom tensorflow.keras.callbacks import Callback\n\nclass TrainingCallback(Callback):\n    def on_train_begin(self, logs=None):\n        print (\"Starting training...\")\n    def on_epoch_begin(self, epoch, logs=None):\n        print(f\"Starting epoch {epoch}\")\n    def on_train_batch_begin(self, batch, logs=None):\n        print(f\"Training: Starting batch {batch}\")\n    def on_train_batch_end(self, batch, logs=None):\n        print(f\"Training: Finished batch {batch}\")\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"Training: Fininshed epoch {epoch}\")    \n    def on_train_end(self, logs=None):\n        print (\"Finished training.\")\n\nclass TestingCallback(Callback):\n    def on_test_begin(self, logs=None):\n        print (\"Starting testing...\")\n    def on_test_batch_begin(self, batch, logs=None):\n        print(f\"Testing: Starting batch {batch}\")\n    def on_test_batch_end(self, batch, logs=None):\n        print(f\"Testing: Finished batch {batch}\")\n    def on_test_end(self, logs=None):\n        print (\"Finished testing.\")\n\nclass PredcitionCallback(Callback):\n    def on_predict_begin(self, logs=None):\n        print (\"Starting predict ing...\")\n    def on_predict_batch_begin(self, batch, logs=None):\n        print(f\"Predicting: Starting batch {batch}\")\n    def on_predict_bach_end(self, batch, logs=None):\n        print(f\"Predicting: Finished batch {batch}\")\n    def on_predict_end(self, logs=None):\n        print (\"Finished predicting.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-build the model\n\nrmodel2 = get_regularised_model(0.00001, 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compile the model\nrmodel2.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mae\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train the model with the callback"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train the model, with some of the data reserved for validation\nhistory = rmodel2.fit(train_data, train_targets, epochs=5, \n                      validation_split=0.15, \n                      batch_size=128, \n                      verbose=0,\n                      callbacks=[TrainingCallback()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model\nrmodel2.evaluate(test_data , test_targets, verbose=0, callbacks=[TestingCallback()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make predictions with the model\n\nrmodel2.predict(test_data, verbose=0, callbacks=[PredcitionCallback()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n<a id=\"coding_tutorial_4\"></a>\n## Early stopping / patience"},{"metadata":{},"cell_type":"markdown","source":"#### Re-train the models with early stopping"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-train the unregularised model\nunregularized_model = get_model()\nunregularized_model.compile(optimizer=\"adam\", loss=\"mse\")\nunreg_history = unregularized_model.fit(\n    train_data, train_targets, \n    epochs=100, validation_split=0.15, batch_size=64,\n    verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model on the test set\nunregularized_model.evaluate(test_data, test_targets, verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-train the regularised model\nregularized_model = get_regularised_model(1e-8, 0.2)\nregularized_model.compile(optimizer=\"adam\", loss=\"mse\")\nreg_history = regularized_model.fit(\n    train_data, train_targets, \n    epochs=100, validation_split=0.15, batch_size=64,\n    verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model on the test set\nregularized_model.evaluate(\n    test_data, test_targets, verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the learning curves"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the training and validation loss\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12, 5))\n\nfig.add_subplot(121)\n\nplt.plot(unreg_history.history['loss'])\nplt.plot(unreg_history.history['val_loss'])\nplt.title('Unregularised model: loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\n\nfig.add_subplot(122)\n\nplt.plot(reg_history.history['loss'])\nplt.plot(reg_history.history['val_loss'])\nplt.title('Regularised model: loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-train the unregularised model with patience\nunregularized_model = get_model()\nunregularized_model.compile(optimizer=\"adam\", loss=\"mse\")\nunreg_history_patience = unregularized_model.fit(\n    train_data, train_targets, \n    epochs=100, validation_split=0.15, batch_size=64,\n    verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-train the regularised model with patience\nregularized_model = get_regularised_model(1e-8, 0.2)\nregularized_model.compile(optimizer=\"adam\", loss=\"mse\")\nreg_history_patience = regularized_model.fit(\n    train_data, train_targets, \n    epochs=100, validation_split=0.15, batch_size=64,\n    verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = plt.figure(figsize=(12, 5))\n\nfig.add_subplot(121)\n\nplt.plot(unreg_history_patience.history['loss'])\nplt.plot(unreg_history_patience.history['val_loss'])\nplt.title('Unregularised model: loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\n\nfig.add_subplot(122)\n\nplt.plot(reg_history_patience.history['loss'])\nplt.plot(reg_history_patience.history['val_loss'])\nplt.title('Regularised model: loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}