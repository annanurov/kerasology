{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Aim\n\nIn this notebook aims to build, compile and fit a neural network model to the Iris dataset. \nWe will implement validation, regularisation and callbacks to improve the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#### PACKAGE IMPORTS ####\n\nfrom numpy.random import seed\nseed(92)\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, model_selection \n\n\n# If you would like to make further imports from tensorflow, add them here\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import initializers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Iris dataset\n\nWe will use the [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). It cantains 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. For a reference, see the following papers:\n\n- R. A. Fisher. \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179â€“188, 1936.\n\nOur goal is to construct a neural network to classifies each sample into the correct class, as well as applying validation and regularisation techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"#To connect to the database while programming in Kaggle, we use the following setup:\ndatafile = \"../input/Iris.csv\"\n#database = \"../input/Database.sqlite\"\n\n#Alternatively, we can use a built-in function `sklearn.datasets.load_iris()`\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_data = datasets.load_iris()\n\ntargets = iris_data.target\ndata = iris_data.data    \n\ntrain_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We will now convert the training and test targets using a one hot encoder.\n\ntrain_targets = tf.keras.utils.to_categorical(np.array(train_targets))\ntest_targets = tf.keras.utils.to_categorical(np.array(test_targets))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Model\n\nWe now define a funtion to return a sample model. \nFollowing are the characteristics of the sample model.\n* The model will use the `input_shape` in the function argument to set the input size in the first layer.\n* The first layer will be a dense layer with 64 units.\n* The weights of the first layer will be initialised with the He uniform initializer.\n* The biases of the first layer will be all initially equal to one.\n* There will be a further four dense layers, each with 128 units.\n* This will be followed with four dense layers, each with 64 units.\n* All of these Dense layers will use the ReLU activation function.\n* The output Dense layer will have 3 units and the softmax activation function.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Sample NN model\n\ndef get_model(input_shape):\n    model=tf.keras.Sequential([\n        Dense(64, activation='relu', input_shape=(input_shape),\n              kernel_initializer=tf.keras.initializers.he_uniform(),\n              bias_initializer=initializers.Ones()),\n        Dense(128,activation='relu'),\n        Dense(128,activation='relu'),\n        Dense(128,activation='relu'),\n        Dense(128,activation='relu'),\n        \n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        \n        Dense(3, activation='softmax')\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model(train_data[0].shape)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Compile the model:\nmodel.compile(loss='mse', optimizer=\"adam\", metrics=[\"mse\",\"mae\",\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting the model\n\nNowwe will train the model on the Iris dataset, using the model's `fit` method. \n* The training will run for a fixed number of epochs, given by the function's `epochs` argument.\n* We will return the training history to be used for plotting the learning curves.\n* We set the batch size to 40 and the validation set to be 15% of the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit( train_data, train_targets, epochs=1000, batch_size=40, validation_split=0.40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now plot two graphs:\n\nEpoch vs accuracy\nEpoch vs loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run this cell to plot the epoch vs loss graph\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Oh No!\n\nIt seems that we have *overfit* our dataset. We will now try to mitigate this overfitting by regularisation.\n\nThe specs for the new regularised model are the same as our original model, with the addition of two dropout layers, weight decay, and a batch normalisation layer. \n\nIn particular, we will\n\n* add a dropout layer after the 3rd Dense layer\n* add two more Dense layers with 128 units before a batch normalisation layer\n* add two more Dense layers with 64 units and then another Dropout layer\n* add two more Dense layers with 64 units and then the final 3-way softmax layer\n* add weight decay (l2 kernel regularisation) in all Dense layers except the final softmax layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dropout\n\ndef get_regularised_model(input_shape, dropout_rate, weight_decay):\n    model=tf.keras.Sequential([\n        Dense(64, activation='relu', input_shape=(input_shape),\n             kernel_initializer=tf.keras.initializers.he_uniform(),\n             bias_initializer=initializers.Ones(),\n             kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(128,activation='relu', kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(128,activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dropout(dropout_rate),\n        Dense(128,activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(128,activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        \n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dropout(dropout_rate),\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(weight_decay)),\n        \n        Dense(3, activation='softmax')\n    ])\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Instantiate, compile and train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model, using a dropout rate of 0.3 and weight decay coefficient of 0.001\n\nreg_model = get_regularised_model(train_data[0].shape, 0.3, 0.001)\n\n# Compile the model\nreg_model.compile(loss='mse', optimizer=\"adam\", metrics=[\"mse\",\"mae\",\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = reg_model.fit( train_data, train_targets, epochs=1000, batch_size=40, validation_split=0.40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run this cell to plot the new accuracy vs epoch graph\n\ntry:\n    plt.plot(history2.history['accuracy'])\n    plt.plot(history2.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(history2.history['acc'])\n    plt.plot(history2.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run this cell to plot the new loss vs epoch graph\n\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the regularisation has helped to reduce the overfitting of the network.\nWe will now incorporate *callbacks* into a new training run that implements early stopping and learning rate reduction on plateaux.\n\nThe function below performs the following:\n\n* It creates an `EarlyStopping` callback object and a `ReduceLROnPlateau` callback object\n* The early stopping callback is used and monitors validation loss with the mode set to `\"min\"` and patience of 30.\n* The learning rate reduction on plateaux is used with a learning rate factor of 0.2 and a patience of 20."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_callbacks():\n\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min')\n    \n    learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20)\n    \n    return early_stopping, learning_rate_reduction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Istantiate the model which incorporates the callbacks:\n\ncall_model = get_regularised_model(train_data[0].shape, 0.3, 0.0001)\ncall_model.compile(loss='mse', optimizer=\"adam\", metrics=[\"mse\",\"mae\",\"accuracy\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nearly_stopping, learning_rate_reduction = get_callbacks()\ncall_history = call_model.fit(train_data, train_targets, epochs=800, validation_split=0.15,\n                         callbacks=[early_stopping, learning_rate_reduction], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run this cell to plot the new accuracy vs epoch graph\n\ntry:\n    plt.plot(call_history.history['accuracy'])\n    plt.plot(call_history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(call_history.history['acc'])\n    plt.plot(call_history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run this cell to plot the new loss vs epoch graph\n\nplt.plot(call_history.history['loss'])\nplt.plot(call_history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model on the test set\nresults = call_model.evaluate(test_data, test_targets, verbose=0)\n#contents of 'results' is loss vaule and  metrics which are [\"mse\",\"mae\",\"accuracy\"], as set during compilie stage.\n\nprint(\"Test loss: {:.3f}\\nTest accuracy: {:.2f}%\".format(results[0], 100 * results[3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following code gives generalized examples of callbacks. Provisous example may be modified accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### Example training callback\n# Write a custom callback\nfrom tensorflow.keras.callbacks import Callback\n\nclass TrainingCallback(Callback):\n    def on_train_begin(self, logs=None):\n        print (\"Starting training...\")\n    def on_epoch_begin(self, epoch, logs=None):\n        print(f\"Starting epoch {epoch}\")\n    def on_train_batch_begin(self, batch, logs=None):\n        print(f\"Training: Starting batch {batch}\")\n    def on_train_batch_end(self, batch, logs=None):\n        print(f\"Training: Finished batch {batch}\")\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"Training: Fininshed epoch {epoch}\")    \n    def on_train_end(self, logs=None):\n        print (\"Finished training.\")\n\nclass TestingCallback(Callback):\n    def on_test_begin(self, logs=None):\n        print (\"Starting testing...\")\n    def on_test_batch_begin(self, batch, logs=None):\n        print(f\"Testing: Starting batch {batch}\")\n    def on_test_batch_end(self, batch, logs=None):\n        print(f\"Testing: Finished batch {batch}\")\n    def on_test_end(self, logs=None):\n        print (\"Finished testing.\")\n\nclass PredcitionCallback(Callback):\n    def on_predict_begin(self, logs=None):\n        print (\"Starting predict ing...\")\n    def on_predict_batch_begin(self, batch, logs=None):\n        print(f\"Predicting: Starting batch {batch}\")\n    def on_predict_bach_end(self, batch, logs=None):\n        print(f\"Predicting: Finished batch {batch}\")\n    def on_predict_end(self, logs=None):\n        print (\"Finished predicting.\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample calls"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = call_model.fit(train_data, train_targets, epochs=5, \n                      validation_split=0.15, \n                      batch_size=128, \n                      verbose=0,\n                      callbacks=[TrainingCallback()])\n# Evaluate the model\ncall_model.evaluate(test_data , test_targets, verbose=0, callbacks=[TestingCallback()])\n\n# Make predictions with the model\ncall_model.predict(test_data, verbose=0, callbacks=[PredcitionCallback()])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample calls"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Training with early stopping:\n# Re-train the regularised model\nhistory = call_model.fit(\n    train_data, train_targets, \n    epochs=100, validation_split=0.15, batch_size=64,\n    verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}